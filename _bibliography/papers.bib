@string{tvcg = {IEEE Transactions on Visualization and Computer Graphics}}

@ARTICLE{wong2024prismatic,
  selected={true},
  my_position={1},
  type={article},
  area={va},
  abbr={Arxiv},
  domain={Fintech},
  publish_year={2024},
  preview={paper-2024-prismatic.png},
  author={Kam-Kwai, Wong and Luo, Yan and Yue, Xuanwu and Chen, Wei and Qu, Huamin},
  title={{Prismatic: Interactive Multi-View Cluster Analysis of Concept Stocks}}, 
  tldr={We used both fundamental and quantitative investing strategies to cluster companies into concept stocks.},
  tldr_image={paper-2024-prismatic-framework.png},
  abstract={Financial cluster analysis allows investors to discover investment alternatives and avoid undertaking excessive risks. However, this analytical task faces substantial challenges arising from many pairwise comparisons, the dynamic correlations across time spans, and the ambiguity in deriving implications from business relational knowledge. We propose Prismatic, a visual analytics system that integrates quantitative analysis of historical performance and qualitative analysis of business relational knowledge to cluster correlated businesses interactively. Prismatic features three clustering processes: dynamic cluster generation, knowledge-based cluster exploration, and correlation-based cluster validation. Utilizing a multi-view clustering approach, it enriches data-driven clusters with knowledge-driven similarity, providing a nuanced understanding of business correlations. Through well-coordinated visual views, Prismatic facilitates a comprehensive interpretation of intertwined quantitative and qualitative features, demonstrating its usefulness and effectiveness via case studies on formulating concept stocks and extensive interviews with domain experts.},
  journal={arXiv preprint arXiv:2402.08978},
  year={2024},
  note={Under Review},
  arxiv={2402.08978},
}

@ARTICLE{zhang2024scrolltimes,
  selected={true},
  my_position={2},
  type={article},
  area={va},
  abbr={PacificVis},
  domain={Digital humanities},
  publish_year={2024},
  preview={paper-2024-scrolltimes.jpg},
  author={Zhang, Wei and Kam-Kwai, Wong and Chen, Yitian and Jia, Ailing and Wang, Luwei and Zhang, Jian-Wei and Cheng, Lechao and Qu, Huamin and Chen, Wei},
  title={ScrollTimes: Tracing the Provenance of Paintings as a Window into History}, 
  tldr={We build biographies for Chinese handscrolls to explore their historical presence and use them as historical narratives.},
  tldr_image={paper-2024-scrolltimes-case.png},
  abstract={The study of cultural artifact provenance, tracing ownership and preservation, holds significant importance in archaeology and art history. Modern technology has advanced this field, yet challenges persist, including recognizing evidence from diverse sources, integrating sociocultural context, and enhancing interactive automation for comprehensive provenance analysis. In collaboration with art historians, we conducted an examination of handscrolls, a traditional Chinese painting form that provides a rich source of historical data and a unique opportunity to explore history through cultural artifacts. We present a three-tiered methodology encompassing Artifact, Contextual, and Provenance levels, designed to create "Biographies" for handscrolls. Our approach incorporates the application of image processing techniques and language models to extract, validate, and augment elements within handscrolls using various cultural heritage databases. To facilitate efficient analysis of non-contiguous extracted elements, we have developed a distinctive layout. Additionally, we introduce ScrollTimes, a visual analysis system tailored to support the three-tiered analysis of handscrolls, allowing art historians to interactively create biographies tailored to their interests. Validated through case studies and expert interviews, our approach offers a window into history, fostering a holistic understanding of handscroll provenance and historical significance.},
  journal=tvcg,
  year={2024},
  pages={1-11},
  arxiv={2306.08834},
  doi={10.1109/TVCG.2024.3388523}
}

@ARTICLE{zhang2024tcp,
  selected={false},
  my_position={3},
  type={article},
  area={va},
  abbr={JCST},
  domain={Digital humanities},
  publish_year={2024},
  preview={paper-2024-tcp-survey.png},
  author={Zhang, Wei and Zhang, Jianwei and Kam-Kwai, Wong and Wang, Yifang and Feng, Yingchaojie and Wang, Luwei and Chen, Wei},
  title={Computational Approaches for Traditional Chinese Painting: From the "Six Principles of Painting" Perspective}, 
  tldr={We modernized a 1500-year-old artistic theory to evaluate the current use of computer technologies on Traditional Chinese Painting, and showed what has been undervalued.},
  tldr_image={paper-2024-tcp-survey-idea.png},
  abstract={Traditional Chinese Painting (TCP) is an invaluable cultural heritage resource and a unique visual art style. In recent years, increasing interest has been placed on digitalizing TCPs to preserve and revive the culture. The resulting digital copies have enabled the advancement of computational methods for structured and systematic understanding of TCPs. To explore this topic, we conducted an in-depth analysis of 92 pieces of literature. We examined the current use of computer technologies on TCPs from three perspectives, based on numerous conversations with specialists. First, in light of the ``Six Principles of Painting" theory, we categorized the articles according to their research focus on artistic elements. Second, we created a four-stage framework to illustrate the purposes of TCP applications. Third, we summarized the popular computational techniques applied to TCPs. The framework also provides insights into potential applications and future prospects, with professional opinion. The list of surveyed publications and related information is available online at https://ca4tcp.com.},
  journal={Journal of Computer Science and Technology},
  year={2024},
  note={To Appear},
  arxiv={2307.14227},
  website={https://ca4tcp.com/},
}

@ARTICLE{weng2024artificialtextdetection,
  selected={false},
  my_position={5},
  type={article},
  area={ai},
  abbr={Info Vis},
  domain={Generative AI},
  publish_year={2024},
  preview={paper-2024-textdetection.png},
  author={Weng, Luoxuan and Liu, Shi and Zhu, Hang and Sun, Jiashun and Kam-Kwai, Wong and Han, Dongming and Zhu, Minfeng and Chen, Wei},
  title={{Towards an Understanding and Explanation for Mixed-Initiative Artificial Scientific Text Detection}}, 
  tldr={We summarized features that tell human-written and AI-generated text apart.},
  tldr_image={paper-2024-textdetection-workflow.png},
  abstract={Large language models (LLMs) have gained popularity in various fields for their exceptional capability of generating human-like text. Their potential misuse has raised social concerns about plagiarism in academic contexts. However, effective artificial scientific text detection is a non-trivial task due to several challenges, including 1) the lack of a clear understanding of the differences between machine-generated and human-written scientific text, 2) the poor generalization performance of existing methods caused by out-of-distribution issues, and 3) the limited support for human-machine collaboration with sufficient interpretability during the detection process. In this paper, we first identify the critical distinctions between machine-generated and human-written scientific text through a quantitative experiment. Then, we propose a mixed-initiative workflow that combines human experts' prior knowledge with machine intelligence, along with a visual analytics prototype to facilitate efficient and trustworthy scientific text detection. Finally, we demonstrate the effectiveness of our approach through two case studies and a controlled user study with proficient researchers. We also provide design implications for interactive artificial text detection tools in high-stakes decision-making scenarios.},
  journal={Information Visualization},
  year={2024},
  arxiv={2304.05011},
  doi={10.1177/14738716241240156}
}

@ARTICLE{wong2023anchorage,
  selected={true},
  my_position={1},
  type={article},
  area={va},
  abbr={TVCG},
  domain={Video analysis},
  publish_year={2023},
  preview={paper-2023-anchorage.png},
  author={Kam-Kwai, Wong and Wang, Xingbo and Wang, Yong and He, Jianben and Zhang, Rong and Qu, Huamin},
  title={Anchorage: Visual Analysis of Satisfaction in Customer Service Videos via Anchor Events}, 
  tldr={We conducted a solid between-subject experiment with 24 domain experts to demonstrate the effectiveness of our approach.},
  abstract={Delivering customer services through video communications has brought new opportunities to analyze customer satisfaction for quality management. However, due to the lack of reliable self-reported responses, service providers are troubled by the inadequate estimation of customer services and the tedious investigation into multimodal video recordings. We introduce Anchorage, a visual analytics system to evaluate customer satisfaction by summarizing multimodal behavioral features in customer service videos and revealing abnormal operations in the service process. We leverage the semantically meaningful operations to introduce structured event understanding into videos which help service providers quickly navigate to events of their interest. Anchorage supports a comprehensive evaluation of customer satisfaction from the service and operation levels and efficient analysis of customer behavioral dynamics via multifaceted visualization views. We extensively evaluate Anchorage through a case study and a carefully-designed user study. The results demonstrate its effectiveness and usability in assessing customer satisfaction using customer service videos. We found that introducing event contexts in assessing customer satisfaction can enhance its performance without compromising annotation precision. Our approach can be adapted in situations where unlabelled and unstructured videos are collected along with sequential records.},
  journal=tvcg,
  year={2023},
  pages={1-13},
  arxiv={2302.06806},
  doi={10.1109/TVCG.2023.3245609},
}

@ARTICLE{feng2023promptmagician,
  selected={true},
  my_position={3},
  type={article},
  area={ai},
  abbr={IEEE VIS},
  domain={Generative AI},
  publish_year={2023},
  preview={paper-2023-promptmagician.png},
  author={Feng, Yingchaojie and Wang, Xingbo and Kam-Kwai, Wong and Wang, Sijia and Lu, Yuhong and Zhu, Minfeng and Wang, Baicheng and Chen, Wei},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={PromptMagician: Interactive Prompt Engineering for Text-to-Image Creation}, 
  tldr={We interviewed Midjourney experts for design implications in supporting Text-to-Image creation and developed a mining-based prompt engineering technique for novices.},
  year={2024},
  volume={30},
  number={1},
  pages={295-305},
  doi={10.1109/TVCG.2023.3327168},
  arxiv={2307.09036},
  git={https://github.com/YingchaojieFeng/PromptMagician}
}

@INPROCEEDINGS{tong2023asymcollab,
  selected={true},
  my_position={3},
  type={inproceedings},
  area={ia},
  abbr={IEEE VR},
  domain={Immersive work environment},
  publish_year={2023},
  preview={paper-2023-asymcollab.png},
  author={Tong, Wai and Xia, Meng and Kam-Kwai, Wong and Bowman, Doug A and Pong, Ting-Chuen and Qu, Huamin and Yang, Yalong},
  title={Towards an Understanding of Distributed Asymmetric Collaborative Visualization on Problem-solving}, 
  tldr={We studied the users' collaborative experience in problem solving where a PC user pairs with a VR user.},
  tldr_image={paper-2023-asymcollab-demo.png},
  abstract={This paper provided empirical knowledge of the user experience for using collaborative visualization in a distributed asymmetrical setting through controlled user studies. With the ability to access various computing devices, such as Virtual Reality (VR) head-mounted displays, scenarios emerge when collaborators have to or prefer to use different computing environments in different places. However, we still lack an understanding of using VR in an asymmetric setting for collaborative visualization. To get an initial understanding and better inform the designs for asymmetric systems, we first conducted a formative study with 12 pairs of participants. All participants collaborated in asymmetric (PC-VR) and symmetric settings (PC-PC and VR-VR). We then improved our asymmetric design based on the key findings and observations from the first study. Another ten pairs of participants collaborated with enhanced PC-VR and PC-PC conditions in a follow-up study. We found that a well-designed asymmetric collaboration system could be as effective as a symmetric system. Surprisingly, participants using PC perceived less mental demand and effort in the asymmetric setting (PC-VR) compared to the symmetric setting (PC-PC). We provided fine-grained discussions about the trade-offs between different collaboration settings.},
  booktitle={IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
  year={2023},
  pages={387-397},
  doi={10.1109/VR55154.2023.00054},
  arxiv={2302.01966}, 
  git={https://github.com/asymcollabvis/asymcollabvis},
}

@ARTICLE{he2023videopro,
  selected={true},
  my_position={3},
  type={article},
  area={ai},
  abbr={IEEE VIS},
  domain={Video analysis},
  publish_year={2023},
  preview={paper-2023-videopro.png},
  author={He, Jianben and Wang, Xingbo and Kam-Kwai, Wong and Huang, Xijie and Chen, Changjian and Chen, Zixin and Wang, Fengjie and Zhu, Min and Qu, Huamin},
  title={VideoPro: A Visual Analytics Approach for Interactive Video Programming}, 
  tldr={We reduced human effort in labeling video data and model steering with flexible, scalable video data programming.},
  tldr_image={paper-2023-videopro-framework.png},
  abstract={Constructing supervised machine learning models for real-world video analysis applications require substantial labeled data, which is costly to acquire due to scarce domain expertise and laborious manual inspection. While data programming shows promise in generating labeled data at scale with user-defined labeling functions, the high dimensional and complex temporal information in videos pose additional challenges in effectively composing and evaluating labeling functions. In this paper, we propose VideoPro, a visual analytics approach to support flexible and scalable video data programming for model steering with reduced human effort. We first extract human-understandable events from videos using computer vision techniques and treat them as atomic components of labeling functions. We further propose a two-stage template mining algorithm that characterizes the sequential patterns of these events to serve as labeling function templates for efficient data labeling. The visual interface of VideoPro facilitates multifaceted exploration, examination, and application of the labeling templates, allowing for effective programming of video data at scale. Moreover, users can monitor the impact of programming on model performance and make informed adjustments during the iterative programming process. We demonstrate the efficiency and effectiveness of our approach with two case studies and expert interviews.},
  journal=tvcg,
  year={2024},
  volume={30},
  number={1},
  pages={87-97},
  doi={10.1109/TVCG.2023.3326586},
  arxiv={2308.00401},
}

@ARTICLE{zhang2023cohortva,
  selected={true},
  my_position={2},
  type={article},
  area={va},
  abbr={IEEE VIS},
  domain={Digital humanities},
  publish_year={2022},
  preview={paper-2022-cohortva.jpg},
  author={Zhang, Wei and Kam-Kwai, Wong and Wang, Xumeng and Gong, Youcheng and Zhu, Rongchen and Liu, Kai and Yan, Zihan and Tan, Siwei and Qu, Huamin and Chen, Siming and Chen, Wei},
  title={CohortVA: A Visual Analytic System for Interactive Exploration of Cohorts based on Historical Data}, 
  tldr={We built knowledge graphs from large-scale databases to study historical figures in cohorts, across space and time.},
  abstract={In history research, cohort analysis seeks to identify social structures and figure mobilities by studying the group-based behavior of historical figures. Prior works mainly employ automatic data mining approaches, lacking effective visual explanation. In this paper, we present CohortVA, an interactive visual analytic approach that enables historians to incorporate expertise and insight into the iterative exploration process. The kernel of CohortVA is a novel identification model that generates candidate cohorts and constructs cohort features by means of pre-built knowledge graphs constructed from large-scale history databases. We propose a set of coordinated views to illustrate identified cohorts and features coupled with historical events and figure profiles. Two case studies and interviews with historians demonstrate that CohortVA can greatly enhance the capabilities of cohort identifications, figure authentications, and hypothesis generation.},
  journal=tvcg,
  year={2023},
  volume={29},
  number={1},
  pages={756-766},
  doi={10.1109/TVCG.2022.3209483},
  arxiv={2208.09237},
  presentation={https://youtu.be/6AJ0C3FZKyY},
}


@ARTICLE{feng2023xnli,
  selected={false},
  my_position={4},
  type={article},
  area={ai},
  abbr={TVCG},
  domain={Natural language interfaces},
  publish_year={2023},
  preview={paper-2023-xnli.png},
  author={Feng, Yingchaojie and Wang, Xingbo and Pan, Bo and Kam-Kwai, Wong and Ren, Yi and Liu, Shi and Yan, Zihan and Ma, Yuxin and Qu, Huamin and Chen, Wei},
  title={XNLI: Explaining and Diagnosing NLI-based Visual Data Analysis}, 
  tldr={We provided explanations for natural language interfaces to help users locate the problems and further revise the queries.},
  abstract={Natural language interfaces (NLIs) enable users to flexibly specify analytical intentions in data visualization. However, diagnosing the visualization results without understanding the underlying generation process is challenging. Our research explores how to provide explanations for NLIs to help users locate the problems and further revise the queries. We present XNLI, an explainable NLI system for visual data analysis. The system introduces a Provenance Generator to reveal the detailed process of visual transformations, a suite of interactive widgets to support error adjustments, and a Hint Generator to provide query revision hints based on the analysis of user queries and interactions. Two usage scenarios of XNLI and a user study verify the effectiveness and usability of the system. Results suggest that XNLI can significantly enhance task accuracy without interrupting the NLI-based analysis process.},
  journal=tvcg,
  year={2023},
  pages={1-14},
  doi={10.1109/TVCG.2023.3240003},
  arxiv={2301.10385},
}

@ARTICLE{zhou2023dpviscreator,
  selected={false},
  my_position={3},
  type={article},
  area={va},
  abbr={IEEE VIS},
  domain={Differential privacy},
  publish_year={2022},
  preview={paper-2022-dpviscreator.png},
  author={Zhou, Jiehui and Wang, Xumeng and Kam-Kwai, Wong and Wang, Huanliang and Wang, Zhongwei and Yang, Xiaoyu and Yan, Xiaoran and Feng, Haozhe and Qu, Huamin and Ying, Haochao and Chen, Wei},
  title={DPVisCreator: Incorporating pattern constraints to privacy-preserving visualizations via differential privacy}, 
  tldr={We generated privacy-protected charts while maintaining user-preferred patterns.},
  abstract={Data privacy is an essential issue in publishing data visualizations. However, it is challenging to represent multiple data patterns in privacy-preserving visualizations. The prior approaches target specific chart types or perform an anonymization model uniformly without considering the importance of data patterns in visualizations. In this paper, we propose a visual analytics approach that facilitates data custodians to generate multiple private charts while maintaining user-preferred patterns. To this end, we introduce pattern constraints to model users' preferences over data patterns in the dataset and incorporate them into the proposed Bayesian network-based Differential Privacy (DP) model PriVis. A prototype system, DPVisCreator, is developed to assist data custodians in implementing our approach. The effectiveness of our approach is demonstrated with quantitative evaluation of pattern utility under the different levels of privacy protection, case studies, and semi-structured expert interviews.},
  journal=tvcg,
  year={2023},
  volume={29},
  number={1},
  pages={809-819},
  doi={10.1109/TVCG.2022.3209391},
  arxiv={2208.13418},
}

@Article{chen2024ethereum,
  selected={false},
  my_position={5},
  type={article},
  area={va},
  abbr={ChinaVis},
  domain={Fintech},
  publish_year={2023},
  preview={paper-2023-ethereum.png},
  author={Chen, Xuan and Zhang, Xincan and Wang, Zhaohan and Yu, Kerun and Kam-Kwai, Wong and Guo, Haoyun and Chen, Siming},
  title={Visual analytics for security threats detection in Ethereum consensus layer},
  tldr={We introduced the first visual analytics solution for security threat awareness on the Ethereum consensus layer.},
  journal={Journal of Visualization},
  year={2024},
  month = {mar},
  abstract={The Ethereum consensus layer provides the Proof of Stake (PoS) consensus algorithm with the beacon chain for the Ethereum blockchain network. However, the beacon chain is proved vulnerable to consensus-targeted attacks, which are difficult to detect. To address this issue, blockchain developers require an interactive tool to identify and mitigate potential security threats. Currently, most blockchain visualization solutions only display client logs or transaction records, making responding quickly to security threats challenging. This paper introduces the first visual analytics solution for security threat awareness on the Ethereum consensus layer. We cooperate with blockchain experts and investigate a top-down exploration approach, providing an overview of the general security level, as well as detailed consensus achievements in each slot. Our visual system lets users discover specific outcomes of the consensus execution and identify anomalies in the beacon chain historical data. Furthermore, the system includes two case studies of actual attacks to help developers better understand and mitigate potential security threats.},
  doi={10.1007/s12650-024-00969-z}
}

@Article{yuan2023taxscheduler,
  selected={false},
  my_position={4},
  type={article},
  area={va},
  abbr={VI},
  domain={Fintech},
  publish_year={2023},
  preview={paper-2023-tax-scheduler.png},
  title = {{Tax-Scheduler: An Interactive Visualization System for Staff Shifting and Scheduling at Tax Authorities}},
  tldr={We developed a visual analytics system for predicting client inflows and scheduling services to maximize productivity.},
  author = {Yuan, Linping and Li, Boyu and Li, Siqi and Kam-Kwai, Wong and Zhang, Rong and Qu, Huamin},
  year = {2023},
  month = {jun},
  journal = {Visual Informatics},
  volume = {7},
  number = {2},
  pages = {30--40},
  issn = {2468-502X},
  doi = {10.1016/j.visinf.2023.02.001},
  abstract = {Given a large number of applications and complex processing procedures, how to efficiently shift and schedule tax officers to provide good services to taxpayers is now receiving more attention from tax authorities. The availability of historical application data makes it possible for tax managers to shift and schedule staff with data support, but it is unclear how to properly leverage the historical data. To investigate the problem, this study adopts a user-centered design approach. We first collect user requirements by conducting interviews with tax managers and characterize their requirements of shifting and scheduling into time series prediction and resource scheduling problems. Then, we propose Tax-Scheduler, an interactive visualization system with a time-series prediction algorithm and genetic algorithm to support staff shifting and scheduling in the tax scenarios. To evaluate the effectiveness of the system and understand how non-technical tax managers react to the system with advanced algorithms and visualizations, we conduct user interviews with tax managers and distill several implications for future system design.},
}

@ARTICLE{yang2023explaining,
  selected={false},
  my_position={3},
  type={article},
  area={va},
  abbr={TVCG},
  domain={Information visualization},
  publish_year={2021},
  preview={paper-2021-examples.png},
  author={Yang, Leni and Xiong, Cindy and Kam-Kwai, Wong and Wu, Aoyu and Qu, Huamin},
  title={Explaining with Examples Lessons Learned from Crowdsourced Introductory Description of Information Visualizations}, 
  tldr={We identified different strategies and best practices for describing data visualizations.},
  abstract={Data visualizations have been increasingly used in oral presentations to communicate data patterns to the general public. Clear verbal introductions of visualizations to explain how to interpret the visually encoded information are essential to convey the takeaways and avoid misunderstandings. We contribute a series of studies to investigate how to effectively introduce visualizations to the audience with varying degrees of visualization literacy. We begin with understanding how people are introducing visualizations. We crowdsource 110 introductions of visualizations and categorize them based on their content and structures. From these crowdsourced introductions, we identify different introduction strategies and generate a set of introductions for evaluation. We conduct experiments to systematically compare the effectiveness of different introduction strategies across four visualizations with 1,080 participants. We find that introductions explaining visual encodings with concrete examples are the most effective. Our study provides both qualitative and quantitative insights into how to construct effective verbal introductions of visualizations in presentations, inspiring further research in data storytelling.},
  journal=tvcg,
  year={2023},
  volume={29},
  number={3},
  pages={1638-1650},
  doi={10.1109/TVCG.2021.3128157},
  arxiv={2112.12364},
}

@article{feng2022ipoet,
  selected={false},
  my_position={4},
  type={article},
  area={va},
  abbr={ChinaVis},
  domain={Digital humanities},
  publish_year={2021},
  preview={paper-2021-ipoet.png},
  abstract={Chinese painting poetry is an extraordinary aesthetic phenomenon in world art history. It is not only part of the paintings but also helps us to better understand the spiritual conception that the artists express. In this paper, we present an interactive visual system to enable ordinary users to compose customized painting poetry for ancient Chinese paintings, which contain three properties: (1) We employ object detection and image captioning to describe the scenery depicted in the painting. (2) We extend the modern color theory to analyze the underlying emotions of each painting. (3) We propose an interactive poetry generation method that takes the content description and the emotional expression to add the diversity of the poetry creation. Several visual components are carefully designed to visualize and contextualize the features in the painting. They effectively guide users to steer the creation of personalized painting poems. We conduct efficient case studies and user interviews to demonstrate the effectiveness of our system.},
  title={iPoet: Interactive Painting Poetry Creation with Visual Multimodal Analysis},
  tldr={We enabled ordinary users to compose customized painting poetry for ancient Chinese paintings.},
  author={Feng, Yingchaojie and Chen, Jiazhou and Huang, Keyu and Kam-Kwai, Wong and Ye, Hui and Zhang, Wei and Zhu, Rongchen and Luo, Xiaonan and Chen, Wei},
  journal={Journal of Visualization},
  year={2022},
  month={jun},
  volume={25},
  number={3},
  pages={671-685},
  doi={10.1007/s12650-021-00780-0},
  arxiv={2307.09036},
}

@article{lin2021taxthemis,
  selected={true},
  my_position={2},
  type={article},
  area={va},
  abbr={IEEE VIS},
  domain={Fintech},
  publish_year={2020},
  preview={paper-2020-taxthemis.png},
  abstract={Tax evasion is a serious economic problem for many countries, as it can undermine the government' s tax system and lead to an unfair business competition environment. Recent research has applied data analytics techniques to analyze and detect tax evasion behaviors of individual taxpayers. However, they failed to support the analysis and exploration of the uprising related party transaction tax evasion (RPTTE) behaviors (e.g., transfer pricing), where a group of taxpayers is involved. In this paper, we present TaxThemis, an interactive visual analytics system to help tax officers mine and explore suspicious tax evasion groups through analyzing heterogeneous tax-related data. A taxpayer network is constructed and fused with the trade network to detect suspicious RPTTE groups. Rich visualizations are designed to facilitate the exploration and investigation of suspicious transactions between related taxpayers with profit and topological data analysis. Specifically, we propose a calendar heatmap with a carefully-designed encoding scheme to intuitively show the evidence of transferring revenue through related party transactions. We demonstrate the usefulness and effectiveness of TaxThemis through two case studies on real-world tax-related data, and interviews with domain experts.},
  title={TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups},
  tldr={We mined suspicious tax evasion groups by analyzing heterogeneous tax-related data.},
  tldr_image={paper-2020-taxthemis-workflow.png},
  author={Lin*, Yating and Kam-Kwai*, Wong and Wang, Yong and Zhang, Rong and Dong, Bo and Qu, Huamin and Zheng, Qinghua},
  journal=tvcg,
  year={2021},
  volume={27},
  number={2},
  pages={849-859},
  doi={10.1109/TVCG.2020.3030370},
  arxiv={2009.03179},
  presentation={https://youtu.be/scTPYya5Rek?t=547},
}